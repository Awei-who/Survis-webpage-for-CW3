@article{Dash2022,
  author    = {Adyasha Dash and Kat R. Agres},
  title     = {AI-Based Affective Music Generation Systems: A Review of Methods, and Challenges},
  journal   = {arXiv.org},
  year      = {2022},
  abstract  = {Music is a powerful medium for altering the emotional state of the listener. In recent years, with significant advancement in computing capabilities, artificial intelligence-based (AI-based) approaches have become popular for creating affective music generation (AMG) systems that are empowered with the ability to generate affective music. Entertainment, healthcare, and sensor-integrated interactive system design are a few of the areas in which AI-based affective music generation (AI-AMG) systems may have a significant impact. Given the surge of interest in this topic, this article aims to provide a comprehensive review of AI-AMG systems. The main building blocks of an AI-AMG system are discussed, and existing systems are formally categorized based on the core algorithm used for music generation. In addition, this article discusses the main musical features employed to compose affective music, along with the respective AI-based approaches used for tailoring them. Lastly, the main challenges and open questions in this field, as well as their potential solutions, are presented to guide future research. We hope that this review will be useful for readers seeking to understand the state-of-the-art in AI-AMG systems, and gain an overview of the methods used for developing them, thereby helping them explore this field in the future.},
  publisher = {Cornell University Library, arXiv.org},
  number    = {1},
  issn      = {2331-8422},
  language  = {eng},
  address   = {Ithaca},
  keywords  = {Affective music, Artificial intelligence, Automatic music generation},
  doi       = {10.48550/arXiv.2301.06890},
}


@article{Santos2021,
  author    = {Luisa Rocha de Azevedo Santos and Carlos Nascimento Silla Jr. and Marjory Da Costa-Abreu},
  title     = {A Methodology for Procedural Piano Music Composition with Mood Templates Using Genetic Algorithms},
  booktitle = {11th International Conference of Pattern Recognition Systems (ICPRS 2021)},
  year      = {2021},
  pages     = {1--6},
  publisher = {IET},
  number    = {1},
  address   = {Sheffield, South Yorkshire, UK},
  url       = {http://shura.shu.ac.uk/28011/},
  abstract  = {In this work, we present a system for generating piano music using mood templates and genetic algorithms. The system evolves a population of random pieces to match the mood of a given template, represented in MIDI format. Human listeners evaluated the generated pieces, and results showed that the music conveyed the intended mood, although it still sounded computer-generated due to rhythmic irregularities.},
  keywords  = {music composition, music emotion, music mood, genetic algorithm, procedural content generation},
  doi       = {10.1049/icp.2021.1435},
}


@article{FerreiraLucasN2020CMfT,
issn = {2331-8422},
abstract = {In this paper we present Bardo Composer, a system to generate background music for tabletop role-playing games. Bardo Composer uses a speech recognition system to translate player speech into text, which is classified according to a model of emotion. Bardo Composer then uses Stochastic Bi-Objective Beam Search, a variant of Stochastic Beam Search that we introduce in this paper, with a neural model to generate musical pieces conveying the desired emotion. We performed a user study with 116 participants to evaluate whether people are able to correctly identify the emotion conveyed in the pieces generated by the system. In our study we used pieces generated for Call of the Wild, a Dungeons and Dragons campaign available on YouTube. Our results show that human subjects could correctly identify the emotion of the generated music pieces as accurately as they were able to identify the emotion of pieces written by humans.},
journal = {arXiv.org},
publisher = {Cornell University Library, arXiv.org},
number = {1},
year = {2020},
title = {Computer-Generated Music for Tabletop Role-Playing Games},
copyright = {2020. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
language = {eng},
address = {Ithaca},
author = {Ferreira, Lucas N and Lelis, Levi H S and Whitehead, Jim},
keywords = {Composers ; Emotions ; Games ; Music ; Role playing ; Speech recognition},
doi       = {https://doi.org/10.1609/aiide.v16i1.7408},
}

@article{FerreiraLucasN2021LtGM,
issn = {2331-8422},
abstract = {Deep Learning models have shown very promising results in automatically composing polyphonic music pieces. However, it is very hard to control such models in order to guide the compositions towards a desired goal. We are interested in controlling a model to automatically generate music with a given sentiment. This paper presents a generative Deep Learning model that can be directed to compose music with a given sentiment. Besides music generation, the same model can be used for sentiment analysis of symbolic music. We evaluate the accuracy of the model in classifying sentiment of symbolic music using a new dataset of video game soundtracks. Results show that our model is able to obtain good prediction accuracy. A user study shows that human subjects agreed that the generated music has the intended sentiment, however negative pieces can be ambiguous.},
journal = {arXiv.org},
publisher = {Cornell University Library, arXiv.org},
number = {1},
year = {2021},
title = {Learning to Generate Music With Sentiment},
copyright = {2021. This work is published under http://creativecommons.org/licenses/by/4.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
language = {eng},
address = {Ithaca},
author = {Ferreira, Lucas N and Whitehead, Jim},
keywords = {Computer & video games ; Data mining ; Deep learning ; Machine learning ; Model accuracy ; Music ; Sound tracks ; Video data},
doi       = {https://doi.org/10.48550/arXiv.2103.06125},
}

@article{Hsiao-TzuHung2021EAMP,
issn = {2331-8422},
abstract = {While there are many music datasets with emotion labels in the literature, they cannot be used for research on symbolic-domain music analysis or generation, as there are usually audio files only. In this paper, we present the EMOPIA (pronounced `yee-m\`{o}-pi-uh') dataset, a shared multi-modal (audio and MIDI) database focusing on perceived emotion in pop piano music, to facilitate research on various tasks related to music emotion. The dataset contains 1,087 music clips from 387 songs and clip-level emotion labels annotated by four dedicated annotators. Since the clips are not restricted to one clip per song, they can also be used for song-level analysis. We present the methodology for building the dataset, covering the song list curation, clip selection, and emotion annotation processes. Moreover, we prototype use cases on clip-level music emotion classification and emotion-based symbolic music generation by training and evaluating corresponding models using the dataset. The result demonstrates the potential of EMOPIA for being used in future exploration on piano emotion-related MIR tasks.},
journal = {arXiv.org},
publisher = {Cornell University Library, arXiv.org},
number = {1},
year = {2021},
title = {EMOPIA: A Multi-Modal Pop Piano Dataset For Emotion Recognition and Emotion-based Music Generation},
copyright = {2021. This work is published under http://creativecommons.org/licenses/by/4.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
language = {eng},
address = {Ithaca},
author = {Hsiao-Tzu Hung and Ching, Joann and Doh, Seungheon and Kim, Nabin and Nam, Juhan and Yang, Yi-Hsuan},
keywords = {Annotations ; Audio data ; Clips ; Datasets ; Emotion recognition ; Emotions ; Labels ; Music ; Pianos},
doi       = {https://doi.org/10.48550/arXiv.2108.01374},
}

@article{Saikia2021,
  author    = {Pragyan Jyoti Saikia and Gunjan Sharma},
  title     = {Emotionally Relevant Background Music Generation for Audiobooks},
  booktitle = {2021 International Conference on Artificial Intelligence and Machine Vision (AIMV)},
  year      = {2021},
  pages     = {20--21},
  publisher = {IEEE},
  number = {1},
  address   = {Sheffield, South Yorkshire, UK},
  doi       = {10.1109/AIMV53313.2021.9670959},
  abstract  = {This paper proposes a system for generating background music for audiobooks that is emotionally relevant to the narrative. The system uses a combination of natural language processing and music generation techniques to align the emotional content of the music with the text. The approach is evaluated using user studies, which show that the generated music enhances the listener's experience by matching the emotional tone of the audiobook.},
  keywords  = {audiobooks, background music, emotion recognition, music generation, natural language processing},
}

@article{ManiktalaMehak2020MPMA,
booktitle = {ACM International Conference Proceeding Series},
isbn = {9781450388078},
year = {2020},
title = {M.I.N.U.E.T.: Procedural Musical Accompaniment for Textual Narratives},
copyright = {Copyright 2022 Elsevier B.V., All rights reserved.},
language = {eng},
number = {1},
author = {Maniktala, Mehak and Miller, Chris and Margolese-Malin, Aaron and Jhala, Arnav and Martens, Chris},
keywords = {mood classification ; music generation ; narrative experience ; procedural content generation ; sentiment analysis},
doi       = {https://doi.org/10.1145/3402942.3409602},
}

@article{SalasJessie2018GMfL,
issn = {0278-6648},
abstract = {This article presents Tambr, a new software for translating literature into sound using multiple synthesized voices selected for the way in which their timbre relates to the meaning and sentiment of the topics conveyed in the story. It achieves this by leveraging a large lexical semantic database to implement a machine-learning-based synthesizer search engine used to select the synthesizers whose meaning best reflects the ideas of the novel. Tambr uses sentiment analysis to generate the pitches, durations, and intervals of the output melodies in a way corresponding to the sentiment of the novel-implementing algorithmic composition of literature-based music at a level of musicality not previously explored.},
journal = {IEEE potentials},
pages = {15--18},
volume = {37},
publisher = {IEEE},
number = {1},
year = {2018},
title = {Generating Music from Literature Using Topic Extraction and Sentiment Analysis},
language = {eng},
address = {New York},
author = {Salas, Jessie},
keywords = {Data mining ; Search engines ; Semantics ; Sentiment analysis ; Software development ; Speech processing ; Synthesis ; Synthesizers ; Timbre},
doi       = {10.1109/MPOT.2016.2550015},
}

@article{Veerendranath2024,
  author    = {Vishruth Veerendranath and Vibha Masti and Utkarsh Gupta and Hrishit Chaudhuri and Gowri Srinivasa},
  title     = {ScripTONES: Sentiment-Conditioned Music Generation for Movie Scripts},
  booktitle = {The Third International Conference on Artificial Intelligence and Machine Learning Systems Proceedings},
  year      = {2024},
  pages     = {1--10},
  publisher = {ACM},
  number = {1},
  address   = {Ithaca, NY},
  doi       = {https://doi.org/10.48550/arXiv.2401.07084},
  abstract  = {This paper presents ScripTONES, a two-stage pipeline for generating music from movie scripts. The first phase performs sentiment analysis of the script using the NRC VAD lexicon, encoding sentiment into the valence-arousal continuous space. The second phase involves conditional music generation using Transformer-based and VAE-based models to create piano MIDI music that matches the extracted sentiment. The system is evaluated through a user survey, showing the effectiveness of the generated music in conveying the intended emotions.},
  keywords  = {sentiment analysis, music generation, movie scripts, artificial intelligence, valence-arousal},
}

@article{Zhao2019,
  author    = {Kun Zhao and Siqi Li and Juanjuan Cai and Hui Wang and Jingling Wang},
  title     = {An Emotional Symbolic Music Generation System based on LSTM Networks},
  booktitle = {2019 IEEE 3rd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)},
  year      = {2019},
  pages     = {2039--2043},
  publisher = {IEEE},
  number = {1},
  address   = {Beijing, China},
  doi       = {10.1109/ITNEC.2019.8729266},
  abstract  = {This work presents an emotional symbolic music generation system using Biaxial LSTM networks. The system introduces the LookBack mechanism to enhance long-term structure in the generated music. The model is conditioned on emotion vectors to generate music that expresses four basic emotions based on Russell's valence-arousal space. Subjective experiments show that the system can generate music with distinguishable emotional expressions.},
  keywords  = {algorithmic composition, music generation, emotional music, neural networks, LSTM},
}
